# GPT-2 Medium (~350M parameters) configuration

model:
  vocab_size: 50257
  context_length: 1024
  n_layers: 24  # More layers than 124M
  n_heads: 16
  d_model: 1024  # Larger hidden dimension
  d_ff: 4096  # 4 * d_model
  dropout: 0.1

training:
  batch_size: 10  # Smaller batch due to larger model
  gradient_accumulation_steps: 8  # Effective batch size = 64
  learning_rate: 3e-4
  warmup_steps: 2000
  max_steps: 100000  # More steps for larger model
  weight_decay: 0.1
  grad_clip: 1.0

  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1e-8

  use_amp: true
  dtype: "bfloat16"

  checkpoint_every: 1000
  eval_every: 500
  log_every: 10

data:
  dataset: "tinystories"
  train_split: "train"
  val_split: "validation"
  num_workers: 4

wandb:
  project: "slm-from-scratch"
  entity: null
  name: "gpt-350m-tinystories"
  tags: ["gpt", "350m", "tinystories"]

modal:
  gpu: "H100"  # Need more VRAM for 350M
  timeout: 43200  # 12 hours max
